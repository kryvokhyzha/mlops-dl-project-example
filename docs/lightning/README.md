[Docs](https://lightning.ai/docs/pytorch/stable/starter/installation.html)

[Lightning Universe](https://github.com/Lightning-Universe) (Комьюнити проекты,
расширения и доп возможности)

[Lightning Flash](https://github.com/Lightning-Universe/lightning-flash)
(Зоопарк готовых моделей)

[Lightning Bolts](https://lightning-bolts.readthedocs.io/en/latest/) (Еще один
зоопарк моделей, датасетов и супер-коллбеков)

[Train model with billions of parameters](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html)

<details markdown="1">
<summary>Не проверял, возможно мусор</summary>

- [Сахарок для S3](https://github.com/Lightning-Universe/AWS-s3_component)
- [Сахарок для Redis](https://github.com/Lightning-Universe/Redis_component)
- [Lightning GPT](https://github.com/Lightning-Universe/lightning-GPT)

</details>

# Intro

Мир обучения нейронок уже давно перешагнул за `train_loop(...)`,
`optimizer.step()`, ..., `zero_grad`, ..., и ровно как вы не дергаете Cuda из
питона и не пишите полносвязанные слои руками: ровно так же не стоит писать
тренировку модели руками, потому что:

- это долго
- это баговано
- каждый раз одно и то же
- множество вещей вы просто не знаете как написать (и это нормально!)
- поддерживать простыню кода в тясячи сток просто невозможно, если вы не
  мазохист
- процесс добавления распределенного обучения для вас станет последней ступенью
  ада

В общем, если вы не получаете удовлетворения от боли и страданий (и даже если
да, то все равно), то призываю использовать готовые системы автоматизии
тренировок.

Далее речь пойдет именно о Lightning, потому что он эффективно решает все
проблемы перечисленные выше.

# Installation

[Installation Guide](https://lightning.ai/docs/pytorch/stable/starter/installation.html)

```bash
# Pip
python -m pip install lightning

# Conda
conda install lightning -c conda-forge
```

# Concepts

Строго говоря, основная конценция заключается в избавлении разработчика от
необходимости писать boilerplate код.

- **Тренировка и Валидация**: Lightning автоматизирует переключение между
  тренировочным и валидационным режимами (а так же test и predict режимы),
  контролирует циклы обучения, и предоставляет полезные коллбеки.
- **GPU и TPU**: Lightning обеспечивает простое управление ресурсами,
  автоматизируя развертывание модели на доступные устройства (GPUs/TPUs).
- **Масштабирование**: с легкостью (реально одной строчкой) можно использовать
  режимы распределенного обучения (и даже инференса)
  - Встроенные стратегии:
    - [DDP](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.strategies.DDPStrategy.html#lightning.pytorch.strategies.DDPStrategy)
    - [DeepSpeed](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.strategies.DeepSpeedStrategy.html#lightning.pytorch.strategies.DeepSpeedStrategy)
    - [Fully Sharded DP](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.strategies.FSDPStrategy.html#lightning.pytorch.strategies.FSDPStrategy)
  - Интеграция со сторонними:
    - [Horovod](https://github.com/Lightning-Universe/lightning-Horovod)
    - [Hive Mind](https://github.com/Lightning-Universe/lightning-Hivemind)
    - [Fairscale](https://github.com/Lightning-Universe/lightning-Fairscale)
    - [Colossal-AI](https://github.com/Lightning-Universe/lightning-ColossalAI)
    - [Bagua](https://github.com/Lightning-Universe/lightning-Bagua)
- **Модульность и переиспользуемость**:
  - `LightningDataModule`: Объединяет в себе весь верхний уровень работы с
    данными (под нижним уровнем понимаются Dataset, Collator, ...). Как и модель
    имеет state_dict, может загружаться в чекпонинт
  - `LightningModule`: Объединяет в себе модель, оптимизатор и логику
    тренировочного цикла. Может быть использован для предсказаний, а также
    сохранен и загружен без проблем.
  - `Callbacks`: Позволяют встраивать дополнительную логику в тренировочный цикл
    без изменения исходного кода модели. А так же есть зоопарк коробчных и
    сторонних колбеков, отвечающих практически за все хуки, которые вам
    захочется добавить
- **Разделение логики**:
  - Research vs Engineering: Lightning позволяет разработчикам сосредотачиваться
    на исследовательской части работы, выделяя логику тренировочного цикла и
    обработку данных в отдельные абстракции.
- **Совместимость и Интеграция**:
  - `LightningModule`: наследуется от torch.nn.Module. Lightning предоставляет
    естественный переход от стандартного кода PyTorch, позволяя легко
    адаптировать существующий код.
  - Совместимость с библиотеками: Обширная поддержка различных библиотек и
    фреймворков для логирования, визуализации и др.
- **Безопасность и Отказоустойчивость**:
  - Остановка и Возобновление: Возможность остановки обучения и последующего
    возобновления с того же места.
  - Очистка: Автоматическое освобождение ресурсов GPU после тренировки.
- **Продвинутые Техники из коробки**:
  - **N-bit Precision**: Простой переход к половинной точности для увеличения
    скорости обучения, сокращения затрат на память и сохранения точности модели.
    - `bf16-mixed`
    - **BitSandBytes**
      ([BNB](https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html#quantization-via-bitsandbytes)):
      - `nf4`: Нормализованный float 4-bit тип данных
      - `nf4-dq`: "dq" расшифровывается как "Double Quantization", что позволяет
        уменьшить средний объем занимаемой памяти за счет квантования констант
        квантования. В среднем это составляет около 0.37 бит на параметр
        (примерно 3 ГБ для 65B модели)
      - `fp4`: Использует стандартный float 4-bit тип данных
      - `fp4-dq`: "dq" расшифровывается как "Double Quantization", что позволяет
        уменьшить средний объем занимаемой памяти за счет квантования констант
        квантования. В среднем это составляет около 0.37 бит на параметр
        (примерно 3 ГБ для 65B модели).
      - `int8`: Использует unsigned int8 тип данных.
      - `int8-training`: int8 для активаций и fp16 для весов.
  - **Gradient Accumulation / Clipping**: Накопление градиентов за несколько
    итераций перед выполнением шага оптимизации для эффективного использования
    ресурсов.
- **On prem clusters и Slurm**:
  [tutorial](https://lightning.ai/docs/pytorch/stable/clouds/cluster_intermediate_1.html)
- **Community and Contribution**: громадный и быстрорастущий комьюнити

# Loggers

- Логгер (или их список) передается напрямую в `pl.Trainer`
- В логгер можно писать через `self.log()` внутри Lightning модулей
- Так же в логгер пишут дополнительные коллбеки и плагины, которые вы передаете
  в `pl.Trainer`
- Некоторые из готовых логгеров:
  - [CSV Logger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.csv_logs.html#module-lightning.pytorch.loggers.csv_logs)
  - [Mlflow Looger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.mlflow.html#module-lightning.pytorch.loggers.mlflow)
  - [Tensorboard Logger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.tensorboard.html#module-lightning.pytorch.loggers.tensorboard)
  - [Wandb Logger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb)

# Callbacks

- Коллебки передатся в `pl.Trainer` и позволяют выполнить какое-то действие в
  конкретный момент процесса обучения, не изменяя исходный код модели.
- Пользователь может создавать собственные коллбеки, оверрайдя методы базового
  `Callback` класса.
- Некоторые полезные коллбеки, которые встроены в Lightning:
  - `ModelCheckpoint`: автоматически сохраняет чекпоинты модели в процессе
    обучения
  - `EarlyStopping`: останавливает обучение, когда заданная метрика перестает
    улучшаться
  - `LearningRateMonitor`: сохраняет значения скорости обучения в логгер
  - `GradientAccumulationScheduler`: накопление градиентов на протяжении
    обучения
  - `BackboneFinetuning`: позволяет тонко настраивать заданные слои модели в
    различных фазах обучения.
  - `StochasticWeightAveraging`: реализует SWA для улучшения устойчивости
    модели.
  - `QuantizationAwareTraining` (выпилили в последних версиях так как часто
    багует, но можно копипастнуть): для обучения, учитывая квантование весов,
    что полезно для деплоя
  - `Pruning`: вы и сами все знаете зачем

# Plugins and Environments

[plugind](https://lightning.ai/docs/pytorch/stable/api_references.html#plugins)
[environments](https://lightning.ai/docs/pytorch/stable/api_references.html#environments)

# FAQ

## Тренировка

### Когда следует использовать аккумуляцию градиентов?

Короткий ответ: всегда.

Даже если в видеопамять помещается необходимый вам размер батча, стоит
попробовать разбить его на 2/4/8/... частей и установить делитель параметров в
GradientAccumulationScheduler. Вероятнее всего вы получите **ощутимый** выигрыш
в скорости, так как вообще говоря шаг оптимайзера занимает достаточно много
времени (это можно узнать передав `profile=True` в `pl.Trainer`). Из минусов то,
что вы получите оврехед на передачу данных с cpu на gpu.

### Какой размер для батча использовать?

Короткий ответ: степень 2, влезающая в видеопамять, но не выше 16384.

Объяснение:

- **Аллокация памяти**: GPU часто эффективнее работают с блоками памяти, размер
  которых является степенью двойки. Таким образом, использование таких размеров
  батча может привести к более оптимальной аллокации памяти и, следовательно,
  более эффективному использованию ресурсов.
- **Параллелизм**: Современные архитектуры GPU оптимизированы для параллельной
  обработки данных, и работа с данными, размер которых является степенью двойки,
  может улучшить параллелизм благодаря упрощению выравнивания памяти.
- **Оптимизации библиотек**: Многие низкоуровневые библиотеки и операции
  (например, кубические перемножения матриц), используемые в машинном обучении,
  оптимизированы для работы с размерами данных, являющимися степенью двойки.

<!--
### Где взять срез полезных рекомендаций по обучению моделек?
Самый короткий ответ: иди на кегл, открывай discussions и сортируй по most votes. Еще вот у [этого](https://www.kaggle.com/thedevastator/) чувака хорошие подборки.


- На сегодняшний день общепринятое мнение в том, что шедулер **One cycle cosine annealing** - самый оптимальный вариант
- Используйте dropout! Добавление dropout между слоями трансформеров обычно приводит к большей стабильности обучения и более робастным результатам
- Dropout также может использоваться для небольшого увеличения производительности
- Для обычных моделей машинного обучения регуляризации L1 или L2 подойдут, но для трансформеров используйте additive и dropout на hidden layers
- используя augmentation данных для валидации или смешивайте предобученные модели с вашей обученной моделью
- Усреднение весов с последних нескольких чекпоинтов модели - реально крутяк. Поэксперементируйте с усредненим только нескольких слоев. Еще через оптюну можно подобрать линейную комбинацию весов для взвешивания чекпоинтов на валидационном сете.
- Крутая и проверенная схема: при валидации или инференсе используйте набор случайных аугментаций (картинок/текстов/прочего...) для каждого семпла несколько раз, затем усредняйте предсказание модели. Это может дать существенный прирост качества
- Эксперементируйте с оптимайзерами:
    - **AdamW** - является улучшением Adam. Предотвращает экспоненциальное затухание весов модели во внешних слоях
    - **Adafactor** - характеризуется низким потреблением памяти и масштабируемостью. Может показать отличную производительность, особенно при использовании с несколькими GPU
    - **Novograd** -  По сути, еще один оптимизатор, похожий на Adam, но с улучшенными свойствами. Использовался для обучения  bert-large
    - **Ranger** - Интересный оптимизатор с хорошими результатами, но, возможно, не так известен.
    - **Lamb** - Оптимизатор для GPU, разработанный победителями конкурсов GLUE и QQP
    - **Lookahead** - Популярный оптимизатор, который можно использовать поверх других оптимизаторов для улучшения производительности
- warmup важен (еще бы)
- Как искать оптимальный LR? Для файнтюна трансформеров начинать с 2e-5 и понижать на экспериментах. Для CNN с 1e-4. -->
